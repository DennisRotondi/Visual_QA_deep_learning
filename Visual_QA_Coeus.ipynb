{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual QA, Coeus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project for the deep learning course 2021/2022 held by professor Fabrizio Silvestri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, read and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code comes partially from the original implementation https://github.com/GT-Vision-Lab/VQA, it helps in reading the json files from the dataset and creating a class to quickly retrieve the annotations of each <question, image> pair. Here we also perform the preprocessing needed for evaluation and on which we build all our work, since it is useful to build the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "class VQA:\n",
    "\tdef __init__(self, annotations_file: str, questions_file: str, images_semi_path: str, remove_mult_ans: bool, limit_size: int):\n",
    "\t\tself.proprocess_init()\n",
    "\t\tdataset = json.load(open(annotations_file, 'r'))\n",
    "\t\tquestions = json.load(open(questions_file, 'r'))\n",
    "\t\tqa = dict()\n",
    "\t\tqqa = dict()\n",
    "\t\t# total number of questions\n",
    "\t\tself.n_tot = 0\n",
    "\t\t# number of answers with more than a word\n",
    "\t\tself.n_mul = 0\n",
    "\t\t# counter for the length of each answer, to understand if a baseline that produce one-word ans is feasible\n",
    "\t\tself.c_len = Counter()\n",
    "\t\tfor ann in dataset['annotations']:\n",
    "\t\t\tself.n_tot += 1\n",
    "\t\t\tif self.n_tot>limit_size:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tqid = ann['question_id']\n",
    "\t\t\tqatmp = ann\n",
    "\t\t\tqatmp[\"multiple_choice_answer\"] = self.preprocess(qatmp[\"multiple_choice_answer\"])\n",
    "\t\t\tln = len(qatmp[\"multiple_choice_answer\"].split())\n",
    "\t\t\tself.c_len.update([ln])\n",
    "\t\t\tif ln > 1:\n",
    "\t\t\t\tself.n_mul += 1\n",
    "\t\t\t\tif remove_mult_ans:\n",
    "\t\t\t\t\tcontinue \n",
    "\t\t\tqa[qid] = qatmp\n",
    "\t\t\tfor i in range(10):\n",
    "\t\t\t\tqa[qid][\"answers\"][i][\"answer\"]=self.preprocess(qa[qid][\"answers\"][i][\"answer\"])\n",
    "\t\tfor ques in questions['questions']:\n",
    "\t\t\tqid = ques['question_id']\n",
    "\t\t\tif qa.get(qid, \"N\") == \"N\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tqqa[qid] = ques\n",
    "\t\t\tqqa[qid][\"question\"]=self.preprocess(qqa[qid][\"question\"])\n",
    "\t\tself.qa = qa\n",
    "\t\tself.qqa = qqa\n",
    "\t\tself.images_semi_path = images_semi_path\n",
    "\n",
    "\tdef get_img_from_id(self, img_id):\n",
    "\t\tids = str(img_id)\n",
    "\t\t# small trick to retrive the path from the id\n",
    "\t\treturn self.images_semi_path+\"0\"*(12-len(ids))+ids+\".jpg\"\n",
    "\n",
    "\tdef proprocess_init(self):\n",
    "\t\t# this function just set the utilities for the preprocessing\n",
    "\t\tself.contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\",\n",
    "                       \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\",\n",
    "                       \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\",\n",
    "                       \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\",\n",
    "                       \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n",
    "                       \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n",
    "                       \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n",
    "                       \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\",\n",
    "                       \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\",\n",
    "                       \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\",\n",
    "                       \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n",
    "                       \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\",\n",
    "                       \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n",
    "                       \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\",\n",
    "                       \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\",\n",
    "                       \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\",\n",
    "                       \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\",\n",
    "                       \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n",
    "                       \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n",
    "                       \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\",\n",
    "                       \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\",\n",
    "                       \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n",
    "\t\tself.manualMap = {'none': '0',\n",
    "                    'zero': '0',\n",
    "                    'one': '1',\n",
    "                    'two': '2',\n",
    "                    'three': '3',\n",
    "                    'four': '4',\n",
    "                    'five': '5',\n",
    "                    'six': '6',\n",
    "                    'seven': '7',\n",
    "                    'eight': '8',\n",
    "                    'nine': '9',\n",
    "                    'ten': '10'\n",
    "                    }\n",
    "\t\tself.articles = ['a', 'an', 'the']\n",
    "\t\tself.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "\t\tself.commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "\t\tself.punct = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                    '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                    '>', '<', '@', '`', ',', '?', '!']\n",
    "\n",
    "\tdef preprocess(self, sentence):\n",
    "\t\tsentence = sentence.lower()\n",
    "\t\tsentence = sentence.replace('\\n', ' ')\n",
    "\t\tsentence = sentence.replace('\\t', ' ')\n",
    "\t\tsentence = sentence.strip()\n",
    "\t\tls = sentence.split()\n",
    "\t\tws = []\n",
    "\t\tfor w in ls:\n",
    "\t\t\tw = self.processPunctuation(w)\n",
    "\t\t\tw = self.processDigitArticle(w)\n",
    "\t\t\tws+=[w]\n",
    "\t\treturn \" \".join(ws)\n",
    "\n",
    "\tdef processPunctuation(self, inText):\n",
    "\t\toutText = inText\n",
    "\t\tfor p in self.punct:\n",
    "\t\t\tif (p + ' ' in inText or ' ' + p in inText) or (re.search(self.commaStrip, inText) != None):\n",
    "\t\t\t\toutText = outText.replace(p, '')\n",
    "\t\t\telse:\n",
    "\t\t\t\toutText = outText.replace(p, ' ')\n",
    "\t\toutText = self.periodStrip.sub(\"\",\n",
    "\t\t\t\t\t\t\t\t\toutText,\n",
    "\t\t\t\t\t\t\t\t\tre.UNICODE)\n",
    "\t\treturn outText\n",
    "\n",
    "\tdef processDigitArticle(self, inText):\n",
    "\t\toutText = []\n",
    "\t\ttempText = inText.lower().split()\n",
    "\t\tfor word in tempText:\n",
    "\t\t\tword = self.manualMap.setdefault(word, word)\n",
    "\t\t\tif word not in self.articles:\n",
    "\t\t\t\toutText.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\tfor wordId, word in enumerate(outText):\n",
    "\t\t\tif word in self.contractions:\n",
    "\t\t\t\toutText[wordId] = self.contractions[word]\n",
    "\t\toutText = ' '.join(outText)\n",
    "\t\treturn outText\n",
    "        \n",
    "annotations_file = \"datasets/v2_mscoco_train2014_annotations.json\"\n",
    "questions_file = \"datasets/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "images_semi_path = \"datasets/train2014/COCO_train2014_\" #000000000009.jpg\n",
    "train_VQA = VQA(annotations_file, questions_file, images_semi_path, remove_mult_ans=True, limit_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we test the above class, with the aim to understand the structure for which we are working on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if it works\n",
    "q = 131366000\n",
    "print(q); print() \n",
    "# print the question\n",
    "print(train_VQA.qa[q]); print()\n",
    "img_id = train_VQA.qqa[q][\"image_id\"]\n",
    "img_pth = train_VQA.get_img_from_id(img_id)\n",
    "print(img_pth); print()\n",
    "\n",
    "img = Image.open(img_pth).convert('RGB')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"QUESTION\")\n",
    "print(train_VQA.qqa[q][\"question\"])\n",
    "print(\"MOST COMMON ANSWER\")\n",
    "print(train_VQA.qa[q][\"multiple_choice_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the vocabs\n",
    "qst_v = Counter()\n",
    "ans_v = Counter()\n",
    "\n",
    "for k in train_VQA.qa:\n",
    "    qst_v.update(train_VQA.qqa[k][\"question\"].split())\n",
    "    ans_v.update(train_VQA.qa[k][\"multiple_choice_answer\"].split())\n",
    "\n",
    "print(qst_v.most_common(10))\n",
    "print(ans_v.most_common(10))\n",
    "\n",
    "\n",
    "def build_vocab(counter, size):\n",
    "    w2id = {}\n",
    "    id2w = {}\n",
    "    for idx, (i, _) in enumerate(counter.most_common(size)):\n",
    "        w2id[i] = idx\n",
    "        id2w[idx] = i\n",
    "    w2id[\"OOV\"] = idx+1\n",
    "    id2w[idx+1] = \"OOV\"\n",
    "    w2id[\"PAD\"] = idx+2\n",
    "    id2w[idx+2] = \"PAD\"\n",
    "    return w2id, id2w\n",
    "\n",
    "\n",
    "# vocabularies size as in the paper\n",
    "qv_size = 1000\n",
    "av_size = 1000\n",
    "\n",
    "qv_w2id, qv_id2w = build_vocab(qst_v, qv_size)\n",
    "av_w2id, av_id2w = build_vocab(ans_v, av_size)\n",
    "\n",
    "print(av_w2id[\"yes\"])\n",
    "print(av_id2w[1001])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def one_group_bar(columns, data, title, percentage=True):\n",
    "  labels = columns\n",
    "  data = data[0]\n",
    "  color_list = []\n",
    "  for _ in range(len(data)):\n",
    "    color = [random.randrange(0, 255)/255, random.randrange(0, 255)/255, random.randrange(0, 255)/255, 1]\n",
    "    color_list.append(color)\n",
    "  x = np.arange(len(labels))\n",
    "  width = 0.5  # the width of the bars\n",
    "  fig, ax = plt.subplots(figsize=(12, 5), layout='constrained')\n",
    "  rects = ax.bar(x, data, width, color=color_list)\n",
    "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "  ax.set_ylabel('Percentage')\n",
    "  ax.set_title(title)\n",
    "  ax.set_xticks(x, labels)\n",
    "  if percentage:\n",
    "    rects_labels = [('%.2f' % i) + \"%\" for i in data]\n",
    "  else:\n",
    "    rects_labels = data\n",
    "  ax.bar_label(rects, rects_labels, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_questions = train_VQA.n_tot\n",
    "number_questions_more_one_word = train_VQA.n_mul\n",
    "number_questions_one_word = total_number_questions - number_questions_more_one_word\n",
    "## 1\n",
    "columns = sorted([e+1 for e in list(train_VQA.c_len.keys())])\n",
    "table_data = list(train_VQA.c_len.values())\n",
    "table_data_percentage = []\n",
    "table_data_percentage.append([(elem/total_number_questions)*100 for elem in table_data])\n",
    "one_group_bar(columns, table_data_percentage, \"Percentage of number of words for all the answers\")\n",
    "\n",
    "## 2\n",
    "columns = [\"1 word\", \"more than 1 word\"]\n",
    "table_data = [number_questions_one_word, number_questions_more_one_word]\n",
    "table_data_percentage = []\n",
    "table_data_percentage.append([(elem/total_number_questions)*100 for elem in table_data])\n",
    "one_group_bar(columns, table_data_percentage, \"Percentage of number of words for all the answers\")\n",
    "\n",
    "## 3\n",
    "q_most_common_words = qst_v.most_common(10)\n",
    "columns = [e[0] for e in q_most_common_words]\n",
    "table_data = [e[1] for e in q_most_common_words]\n",
    "one_group_bar(columns, [table_data], \"Count of the most predominant words in questions\", False)\n",
    "\n",
    "## 4\n",
    "a_most_common_words = ans_v.most_common(10)\n",
    "columns = [e[0] for e in a_most_common_words]\n",
    "table_data = [e[1] for e in a_most_common_words]\n",
    "table_data_percentage = []\n",
    "table_data_percentage.append([(elem/total_number_questions)*100 for elem in table_data])\n",
    "one_group_bar(columns, [table_data], \"Count of the most predominant words in answers\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, List, Any, Dict, Optional\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass, asdict\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HParams():\n",
    "    # dataset stuff\n",
    "    batch_size: int = 128  # 128 for 1234, 256 for 34, 256 for 234\n",
    "    n_cpu: int = 8\n",
    "    qv_size: int = 1000\n",
    "    av_size: int = 1000\n",
    "    lr: int = 1e-3\n",
    "    wd: int = 0\n",
    "    embedding_dim: int = 768\n",
    "    hidden_dim: int = 400\n",
    "    bidirectional: bool = True\n",
    "    num_layers: int = 1\n",
    "    dropout: float = 0.3\n",
    "    trainable_embeddings: bool = True\n",
    "\n",
    "\n",
    "hparams = asdict(HParams())\n",
    "\n",
    "\n",
    "class VQA_Dataset(Dataset):\n",
    "    def __init__(self, VQA, qv_w2id, av_w2id):\n",
    "        self.data = self.make_data(VQA, qv_w2id, av_w2id)\n",
    "\n",
    "    def make_data(self, VQA, qv_w2id, av_w2id):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            # https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        data = list()\n",
    "        qv_l = len(qv_w2id.keys())\n",
    "        av_l = len(av_w2id.keys())\n",
    "        # efficient handling of the images\n",
    "        imgs = dict()\n",
    "        for k in VQA.qa:\n",
    "            # we build for each question the item <qst, img, ans> for the training phase\n",
    "            item = dict()\n",
    "            item[\"id\"] = k\n",
    "            item[\"input\"] = [qv_w2id.get(w, qv_l)\n",
    "                             for w in VQA.qqa[k]['question'].split()]\n",
    "            img_id = VQA.qqa[k][\"image_id\"]\n",
    "            img_pth = VQA.get_img_from_id(img_id)\n",
    "            imgs[img_id] = imgs.get(img_id, Image.open(img_pth).convert('RGB'))\n",
    "            item[\"img\"] = transform(imgs[img_id])\n",
    "            if (item[\"img\"].shape[0] < 3):\n",
    "                print(k)\n",
    "            item[\"output\"] = torch.as_tensor(av_w2id.get(\n",
    "                VQA.qa[k][\"multiple_choice_answer\"], av_l))\n",
    "            data.append(item)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class VQA_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams: dict, tran_VQA: Any, val_VQA: Any, qv_w2id: dict, qv_id2w: dict, av_w2id: dict, av_id2w: dict) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.val_VQA = val_VQA\n",
    "        self.val_VQA = val_VQA\n",
    "        self.qv_w2id = qv_w2id\n",
    "        self.qv_id2w = qv_id2w\n",
    "        self.av_w2id = av_w2id\n",
    "        self.av_id2w = av_id2w\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.data_train = VQA_Dataset(\n",
    "            self.train_VQA, self.qv_w2id, self.av_w2id)\n",
    "        self.data_val = VQA_Dataset(self.val_VQA, self.qv_w2id, self.av_w2id)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # change collate based on the task\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.n_cpu,\n",
    "            collate_fn=self.collate,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # change collate based on the task\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.n_cpu,\n",
    "            collate_fn=self.collate,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def collate(self, batch):\n",
    "        batch_out = dict()\n",
    "        batch_out[\"id\"] = [sample[\"id\"] for sample in batch]\n",
    "        batch_out[\"output\"] = torch.as_tensor(\n",
    "            [sample[\"output\"] for sample in batch])\n",
    "        batch_out[\"img\"] = torch.stack(\n",
    "            [sample[\"img\"] for sample in batch], dim=0)\n",
    "        batch_out[\"input\"] = pad_sequence(\n",
    "            [torch.as_tensor(sample[\"input\"]) for sample in batch],\n",
    "            batch_first=True,\n",
    "            padding_value=self.hparams.qv_size+1\n",
    "        )\n",
    "        return batch_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file_val = \"datasets/v2_mscoco_val2014_annotations.json\"\n",
    "questions_file_val = \"datasets/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "images_semi_path_val = \"datasets/val2014/COCO_val2014_\"\n",
    "\n",
    "val_VQA = VQA(annotations_file_val, questions_file_val, images_semi_path_val, remove_mult_ans=False, limit_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = VQA_DataModule(hparams, train_VQA, val_VQA, qv_w2id, qv_id2w, av_w2id, av_id2w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47190f7cb2772d39fc79edc4ccd16e1d5aa2f435aeaf1de6cf1f85c03f1e696f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
